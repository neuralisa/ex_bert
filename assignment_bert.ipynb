{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8_wEtBmL1Hwk"
      },
      "source": [
        "# Assignment - Transformer and BERT\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OTZtiOeg1vHP"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lL9OYI1C1wRq"
      },
      "source": [
        "Fill your information here & run the cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QilVSubqFXn4"
      },
      "source": [
        "## Transformer and BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RBrRyCdUFhlj"
      },
      "source": [
        "In this assignment, you will:\n",
        "- Implement a simplified BERT from scratch\n",
        "- Visualize attention in your implementd model\n",
        "- Fine-tune a pre-trained BERT model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gVQ9Q4lw4_XA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g7xU1WDCtLvd"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MyRw05ButO-4"
      },
      "source": [
        "In order to implement BERT, we should first implement the encoder layer of the transformer. An encoder has 2 main sub-layers: multi-headed attention layer and a simple feed-forward layer. The multi-headed attention layer is already implemented , but you should implement the feedforward sub-layer \n",
        "<br>\n",
        "<center>\n",
        "\n",
        "![](https://github.com/iust-deep-learning/982/raw/master/static_files/assignments/asg04_assets/encoder.PNG)\n",
        "\n",
        "</center>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "prl-PotP4gNl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ------------------------------ Encoder ------------------------------\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.projection_dim = hidden_size // num_heads\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def attention(self, query, key, value, mask):\n",
        "        # print(f'Query shape before reshape: {query.shape}')\n",
        "        # print(f'Key shape before reshape: {key.shape}')\n",
        "\n",
        "        query = query.reshape(query.shape[0], query.shape[1], -1, self.projection_dim)\n",
        "        key = key.reshape(key.shape[0], key.shape[1], -1, self.projection_dim)\n",
        "        value = value.reshape(value.shape[0], value.shape[1], -1, self.projection_dim)\n",
        "\n",
        "        score = torch.matmul(query, key.transpose(-2, -1))\n",
        "        dim_key = torch.tensor(key.shape[-1], dtype=torch.float32)\n",
        "        scaled_score = score / torch.sqrt(dim_key)\n",
        "\n",
        "        if mask is not None:\n",
        "            # Change from mask.unsqueeze(1) to mask.unsqueeze(1).unsqueeze(2)\n",
        "            # to add an extra dimension for the heads and then expand to match the score dimensions\n",
        "            mask = mask.unsqueeze(1).unsqueeze(2)  # Add a new dimension for heads\n",
        "            mask = mask.expand_as(scaled_score)  # Ensure mask is correctly expanded\n",
        "            masked_score = scaled_score.masked_fill(mask == 0, -1e9)\n",
        "        else:\n",
        "            masked_score = scaled_score\n",
        "\n",
        "        weights = F.softmax(masked_score, dim=-1)\n",
        "        output = torch.matmul(weights, value)\n",
        "\n",
        "        #print(f'Output shape after matmul: {output.shape}')\n",
        "        return output, weights\n",
        "\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = x.reshape(batch_size, -1, self.num_heads, self.projection_dim)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, inputs, att_mask):\n",
        "        batch_size = inputs.shape[0]\n",
        "        query = self.separate_heads(self.Q(inputs), batch_size)\n",
        "        key = self.separate_heads(self.K(inputs), batch_size)\n",
        "        value = self.separate_heads(self.V(inputs), batch_size)\n",
        "\n",
        "        # print(f'Separate heads shapes - Query: {query.shape}, Key: {key.shape}, Value: {value.shape}')\n",
        "\n",
        "        attention, self.att_weights = self.attention(query, key, value, att_mask)\n",
        "        attention = attention.permute(0, 2, 1, 3)\n",
        "\n",
        "        # print(f'Attention shape after permute: {attention.shape}')\n",
        "\n",
        "        concat_attention = attention.reshape(batch_size, -1, self.hidden_size)\n",
        "\n",
        "        # print(f'Concat attention shape: {concat_attention.shape}')\n",
        "\n",
        "        output = self.out(concat_attention)\n",
        "\n",
        "        # print(f'Output shape: {output.shape}')\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "558gGbanoBsu"
      },
      "source": [
        "**Question**: Why does the transformer use multi-headed attention instead of just a single self-attention?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r4DkUpBToQ8v"
      },
      "source": [
        "<font color=red> Write your answer here</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KCVLJBv951nW"
      },
      "source": [
        "#### Feed-Forward Sub-Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FRnFXb0WwwSB"
      },
      "source": [
        "The feed-forward sub-layer of the encoder has two dense layers. The first dense layer is called the \"intermediate\" layer and the second one is the \"output\" layer whose functionality is to down-project back to the hidden layer size. Dropout is also applied to the output of the intermediate layer. Unlike the original transformer, BERT uses \"GELU\" activation function in the intermediate dense layer. Since there is no GELU activation function in TensorFlow (there is one in TensorFlow Addons but it will crash your session!), you should implement it yourself!\n",
        "\n",
        "Here is the GELU paper: https://arxiv.org/abs/1606.08415 . Or you can just search the internet!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qzD7BjELQ--j"
      },
      "outputs": [],
      "source": [
        "\n",
        "def GELU(x):\n",
        "\n",
        "  ########################################\n",
        "  #     Put your implementation here     #\n",
        "  ########################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Gqd6wedZXxzD"
      },
      "outputs": [],
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__(self, intermediate_size, hidden_size, drop_rate):\n",
        "        super(FFN, self).__init__()\n",
        "        self.intermediate = nn.Linear(hidden_size, intermediate_size)\n",
        "        self.out = nn.Linear(intermediate_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.intermediate(inputs)\n",
        "        x = \n",
        "        x = \n",
        "        x = self.out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aDlwb3Ea6Aqc"
      },
      "source": [
        "#### Residual Connections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K4-UMLpDUkFa"
      },
      "source": [
        "In the encoder, dropout is applied to each sub-layer's output, then it gets added to the sub-layer's input (residual connection) and finaly goes through a layer normalizaion step. You should implement all the aforementioned steps in the **AddNorm** custom layer in the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_TtnesNMOHUF"
      },
      "outputs": [],
      "source": [
        "class AddNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, LNepsilon, drop_rate):  # Add hidden_size\n",
        "        super(AddNorm, self).__init__()\n",
        "        self.LN = nn.LayerNorm(hidden_size, eps=LNepsilon)  # Correctly use hidden_size\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, sub_layer_in, sub_layer_out):\n",
        "          x = self.dropout(sub_layer_out)\n",
        "\n",
        "          ########################################\n",
        "          #     Put your implementation here     #\n",
        "          ########################################\n",
        "          x = \n",
        "          x = self.LN(x)\n",
        "          return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AKqyg0J_WuTv"
      },
      "source": [
        "Now we have everything we need to implement an encoder layer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "16zvGFBo_uaQ"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads, intermediate_size, drop_rate=0.1, LNepsilon=1e-12):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.attention = MultiHeadAttention(hidden_size, num_heads)\n",
        "        self.ffn = FFN(intermediate_size, hidden_size, drop_rate)\n",
        "        self.addnorm1 = AddNorm(hidden_size, LNepsilon, drop_rate)  # Pass hidden_size\n",
        "        self.addnorm2 = AddNorm(hidden_size, LNepsilon, drop_rate)  # Pass hidden_size\n",
        "\n",
        "    def forward(self, inputs, mask):\n",
        "        att_out = self.attention(inputs, mask)\n",
        "        att_out = self.addnorm1(inputs, att_out)\n",
        "         ########################################\n",
        "          #     Put your implementation here     #\n",
        "          ########################################\n",
        "        ffn_out = \n",
        "        ffn_out = \n",
        "        return ffn_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "umQ878ho-6Hp"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DLPlDTeRXefl"
      },
      "source": [
        "In the previous part, you implemented the encoder layer. We only need two more layers to implement BERT. First layer is the embedding layer. The final embedding for each token in BERT is the addition of three types of embeddings. Aside from token embeddings, there is also segment embeddings and position embeddings. For this assignment we are ignoring the segment embeddings since we only want to do single sentence classification! <br>\n",
        "Unlike the transformer, which uses fixed positional embeddings, BERT uses learned positional embeddings.\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "\n",
        "![](https://github.com/iust-deep-learning/982/raw/master/static_files/assignments/asg04_assets/bert_emb.PNG)\n",
        "\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "Note that layer normalization followed by dropout is applied to the final embeddings (after adding all the embeddings).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VCX-g4qjojYr"
      },
      "source": [
        "**Question**: What is segment embedding's functionality in BERT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4I0E6_AOozvd"
      },
      "source": [
        "<font color=red> Write your answer here</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DTW-F4t9_x24"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ------------------------------ BERT ------------------------------\n",
        "\n",
        "class BertEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, maxlen, hidden_size):\n",
        "        super(BertEmbedding, self).__init__()\n",
        "        self.TokEmb = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
        "        self.PosEmb = nn.Parameter(torch.randn(maxlen, hidden_size))\n",
        "        self.LN = nn.LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        tok_emb = self.TokEmb(inputs)\n",
        "        pos_emb = self.PosEmb[:inputs.shape[1], :]\n",
        "        emb = tok_emb + pos_emb\n",
        "        emb = self.LN(emb)\n",
        "        emb = self.dropout(emb)\n",
        "        return emb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kPjWqcH-ytQP"
      },
      "source": [
        "The last layer you need to implement is the \"pooler\". The pooler converts the hidden states of the last encoder layer (which is of shape **[batch_size, sequence_lenght, hidden_size]**) to a vector representation (which is of shape **[batch_size, hidden_size]**) for each input sentence. The pooler does this by simply taking the hidden state corresponding to the first token (a special token in the beggining of each sentence) and feeding it to a dense layer (tanh is used as the activation function of this dense layer in the original implementation). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "O719umhMz_UH"
      },
      "outputs": [],
      "source": [
        "class Pooler(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Pooler, self).__init__()\n",
        "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, encoder_out):\n",
        "        first_token = encoder_out[:, 0, :]\n",
        "        pooled_out = self.dense(first_token)\n",
        "        return pooled_out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5FsjGcFxo8JU"
      },
      "source": [
        "**Question**: As it was explained earlier, the pooler's job is to create a single vector representation of a sentence (or sentence pair) by taking the hidden state corresponding to the first token. Can you suggest another form of pooling that could work for BERT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QvR3jJIEpkka"
      },
      "source": [
        "<font color=red> Write your answer here</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-P8zt_tFojZY"
      },
      "source": [
        "Now you can use the the **create_BERT** function in the cell below. This function gets BERT's hyper-parameters as its inputs and return a BERT model. Use the functional api to create the model.<br>\n",
        "Note that the returned model must have two outputs (just like the pre-trained BERTs): \n",
        "- The hidden states of the last encoder layer\n",
        "- Output of the pooler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "L9tD7UtNfZ4p"
      },
      "outputs": [],
      "source": [
        "def create_BERT(vocab_size, maxlen, hidden_size, num_layers, num_att_heads, intermediate_size, drop_rate=0.1):\n",
        "    \"\"\"\n",
        "    Creates a BERT model based on the arguments provided.\n",
        "\n",
        "    Arguments:\n",
        "    vocab_size: number of words in the vocabulary\n",
        "    maxlen: maximum length of each sentence\n",
        "    hidden_size: dimension of the hidden state of each encoder layer\n",
        "    num_layers: number of encoder layer\n",
        "    num_att_heads: number of attention heads in the multi-headed attention layer\n",
        "    intermediate_size: dimension of the intermediate layer in the feed-forward sublayer of the encoders\n",
        "    drop_rate: dropout rate of all the dropout layers used in the model\n",
        "    returns:\n",
        "    model: a BERT model\n",
        "    \"\"\"\n",
        "\n",
        "    emb = BertEmbedding(vocab_size, maxlen, hidden_size)\n",
        "    encoder_layers = nn.ModuleList([Encoder(hidden_size, num_att_heads, intermediate_size, drop_rate, LNepsilon=1e-12) for _ in range(num_layers)])  # Pass LNepsilon\n",
        "    pooler = Pooler(hidden_size)\n",
        "\n",
        "    model = nn.ModuleList([emb, encoder_layers, pooler])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PKBEBTI6sFKu"
      },
      "source": [
        "The Rotten tomatoes critic reviews dataset is used for this assignment. This dataset consists of about 350000 short reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "X0DJ9nBtXIJi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IUn-48AVXbfR"
      },
      "outputs": [],
      "source": [
        "train_reviews, test_reviews = pd.read_csv('train_reviews.csv').values[:, 1:], pd.read_csv('test_reviews.csv').values[:, 1:]\n",
        "\n",
        "(train_texts, train_labels), (test_texts, test_labels) = (train_reviews[:, 0], train_reviews[:, 1]), (test_reviews[:, 0], test_reviews[:, 1])\n",
        "\n",
        "train_texts = [s.lower() for s in train_texts]\n",
        "test_texts = [s.lower() for s in test_texts]\n",
        "\n",
        "aprx_vocab_size = 20000\n",
        "cls_token = '[cls]'\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Using pre-trained BERT tokenizer\n",
        "MAXLEN = 32\n",
        "\n",
        "class ReviewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoded_text = tokenizer.encode_plus(text, add_special_tokens=True, max_length=MAXLEN, padding='max_length', truncation=True, return_tensors='pt')\n",
        "        input_ids = encoded_text['input_ids'].squeeze()\n",
        "        attention_mask = encoded_text['attention_mask'].squeeze()\n",
        "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': torch.tensor(label, dtype=torch.long)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UhTvBa9ntO7b"
      },
      "source": [
        "Now complete the **encode_sentence** function in the cell below. This function recieves a sentence and an integer denoting the maximum length of the sentence as inputs and returns a list of token ids. Here are the steps to implement this function:\n",
        "- encode the input sentence using the trained tokenizer to receive a token id list\n",
        "- zero-pad the token id list to the maximum length\n",
        "- add the id corresponding to the special token to the beggining of the token id list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JBirx1Fbvv-k"
      },
      "source": [
        "Now use the functional api and the **create_BERT** function you implemented earlier to create a classifier for the movie reviews dataset.\n",
        "Note that the intermediate layer in the feed-forward sub-layer of the encoders is set to $4\\times H$ in the original BERT implementation, where $H$ is the hidden layer size. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SZOW4L9gBqvc"
      },
      "outputs": [],
      "source": [
        "# ------------------------------ Training ------------------------------\n",
        "\n",
        "train_dataset = ReviewsDataset(train_texts, train_labels)\n",
        "test_dataset = ReviewsDataset(test_texts, test_labels)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "# Using pre-trained BERT-Base hyperparameters\n",
        "hidden_size = 768\n",
        "num_heads = 6\n",
        "num_layers = 6\n",
        "\n",
        "# Creating BERT model\n",
        "model = create_BERT(vocab_size=tokenizer.vocab_size, maxlen=MAXLEN, hidden_size=hidden_size, num_layers=num_layers, num_att_heads=num_heads, intermediate_size=4*hidden_size)\n",
        "\n",
        "# Defining optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "# use tqdm for progress bar\n",
        "batch_number = 0\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        batch_number += 1\n",
        "        #print(f'Batch % completed : {batch_number/len(train_dataloader)}')\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Pass input through the model\n",
        "        emb = model[0](input_ids)\n",
        "        for encoder in model[1]:\n",
        "            emb = encoder(emb, attention_mask)\n",
        "        output = model[2](emb)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_fn(output, labels)\n",
        "\n",
        "        # Backpropagate and update weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss = {total_loss/len(train_dataloader)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# write an evaluation function for the test data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "assignment_4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
